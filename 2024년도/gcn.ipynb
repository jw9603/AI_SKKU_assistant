{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLQ5XtaxHHYZ","executionInfo":{"status":"ok","timestamp":1704780282928,"user_tz":-540,"elapsed":1024,"user":{"displayName":"정지원","userId":"16823124283758121408"}},"outputId":"b1c58f1d-3cbf-4d49-dbc9-30027b07a971"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Jan  9 06:04:41 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   43C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUv7M0dIhLA2","outputId":"aba8efc6-a190-4e1e-b0ee-c51d114b4b53","executionInfo":{"status":"ok","timestamp":1704780315742,"user_tz":-540,"elapsed":31565,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from __future__ import division\n","from __future__ import print_function\n","\n","import time\n","\n","import math\n","import torch\n","\n","import numpy as np\n","import scipy.sparse as sp\n","import torch.optim as optim\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module"],"metadata":{"id":"KIdGIgUfjO26","executionInfo":{"status":"ok","timestamp":1704780329011,"user_tz":-540,"elapsed":3460,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# layers\n","class GraphConvolution(Module):\n","    \"\"\"\n","    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n","    \"\"\"\n","\n","    def __init__(self, in_features, out_features, bias=True):\n","        super(GraphConvolution, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n","        # This is not a linear layer, but literally has only parameter values.\n","        if bias:\n","            self.bias = Parameter(torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias', None)\n","        self.reset_parameters() # weight paremeters initialization\n","\n","    def reset_parameters(self):\n","        stdv = 1. / math.sqrt(self.weight.size(1))\n","        self.weight.data.uniform_(-stdv, stdv) # uniform distribution U(-stdv,stdv)\n","        if self.bias is not None:\n","            self.bias.data.uniform_(-stdv, stdv)\n","\n","    def forward(self, input, adj):\n","        # matrix multiplication\n","        support = torch.mm(input, self.weight)\n","        # matrix multiplication of the sparse matrix adj and the support matrix.\n","        output = torch.spmm(adj, support)\n","        if self.bias is not None:\n","            return output + self.bias\n","        else:\n","            return output\n","\n","    def __repr__(self):\n","        return self.__class__.__name__ + ' (' \\\n","               + str(self.in_features) + ' -> ' \\\n","               + str(self.out_features) + ')'"],"metadata":{"id":"0pXsGVlQkI_U","executionInfo":{"status":"ok","timestamp":1704780329011,"user_tz":-540,"elapsed":3,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# model\n","\n","class GCN(nn.Module):\n","    def __init__(self, nfeat, nhid, nclass, dropout):\n","        super(GCN, self).__init__()\n","\n","        self.gc1 = GraphConvolution(nfeat, nhid)\n","        self.gc2 = GraphConvolution(nhid, nclass)\n","        self.dropout = dropout\n","\n","    def forward(self, x, adj):\n","        x = F.relu(self.gc1(x, adj))\n","        # print('X1',x.shape)    # torch.Size([2708, 16])\n","        # As for the variable called training, whenever the mode is changed by calling the model.train() or model.eval() function, self.training changes to True or False.\n","        x = F.dropout(x, self.dropout, training=self.training)\n","        x = self.gc2(x, adj)\n","        # print('X2',x.shape)   # torch.Size([2708, 7])\n","\n","        return F.log_softmax(x, dim=1) # The nll_loss does not have log and softmax applied, hence using log_softmax."],"metadata":{"id":"RkVgJet0kXpO","executionInfo":{"status":"ok","timestamp":1704780329011,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# utils\n","def encode_onehot(labels):\n","\n","    classes = set(labels)\n","    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n","                    enumerate(classes)}\n","\n","    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n","                             dtype=np.int32)\n","    return labels_onehot\n","\n","\n","def load_data(path=\"/content/drive/MyDrive/GCN_exercise/data/cora/\", dataset=\"cora\"):\n","    print('Loading {} dataset...'.format(dataset))\n","\n","    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n","                                        dtype=np.dtype(str))\n","    # print('idx_features_labels',idx_features_labels.shape) # (2708, 1435)\n","    # print('first',idx_features_labels[:,0])                # id\n","    # print('last',idx_features_labels[:,-1])                # label, e.g. ['Neural_Networks' 'Rule_Learning' 'Reinforcement_Learning' ...'Genetic_Algorithms' 'Case_Based' 'Neural_Networks']\n","\n","    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n","    labels = encode_onehot(idx_features_labels[:, -1])\n","    # print('features',features.shape)                       # (2708,1433)\n","    # print('labels',labels.shape)                           # (2708, 7)\n","\n","    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n","\n","    idx_map = {j: i for i, j in enumerate(idx)}\n","    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),\n","                                    dtype=np.int32)\n","\n","    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n","                     dtype=np.int32).reshape(edges_unordered.shape)\n","    # |edges| = (edges개수=5429, 2)\n","    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),\n","                        shape=(labels.shape[0], labels.shape[0]),\n","                        dtype=np.float32) # |labels| =(2708,8), 2708 = # data\n","\n","    #  symmetric adjacency matrix\n","    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n","    # print('adj',adj.shape) # (2708, 2708)\n","\n","    features = normalize(features)\n","    adj = normalize(adj + sp.eye(adj.shape[0])) # Add self loop to adjacency matrix\n","\n","\n","    idx_train = range(140)\n","    idx_val = range(200, 500)\n","    idx_test = range(500, 1500)\n","\n","    features = torch.FloatTensor(np.array(features.todense()))\n","\n","    labels = torch.LongTensor(np.where(labels)[1])\n","\n","    adj = sparse_mx_to_torch_sparse_tensor(adj)\n","\n","\n","    # train, val, test에 index를 지정하여 semi-supervised(transductive)를 사용하고자 함\n","    idx_train = torch.LongTensor(idx_train)\n","    idx_val = torch.LongTensor(idx_val)\n","    idx_test = torch.LongTensor(idx_test)\n","\n","    return adj, features, labels, idx_train, idx_val, idx_test\n","\n","def normalize(mx):\n","    \"\"\"Row-normalize sparse matrix\"\"\"\n","    rowsum = np.array(mx.sum(1))            # (2708,1)\n","    r_inv = np.power(rowsum, -1).flatten()  # (2708, )\n","\n","    r_inv[np.isinf(r_inv)] = 0.\n","    r_mat_inv = sp.diags(r_inv)\n","    mx = r_mat_inv.dot(mx)\n","    return mx\n","\n","def accuracy(output, labels):\n","\n","    preds = output.max(1)[1].type_as(labels)\n","    correct = preds.eq(labels).double()\n","    correct = correct.sum()\n","    return correct / len(labels)\n","\n","def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n","    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n","    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n","    indices = torch.from_numpy(\n","        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n","    values = torch.from_numpy(sparse_mx.data)\n","    shape = torch.Size(sparse_mx.shape)\n","    return torch.sparse.FloatTensor(indices, values, shape)"],"metadata":{"id":"H8aZKCUxkcjB","executionInfo":{"status":"ok","timestamp":1704780329011,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Training\n","import easydict\n","\n","args = easydict.EasyDict({\"no-cuda\":False, \"fastmode\":False, \"seed\":42, \\\n","                          \"epochs\":200, \"lr\":0.01, \"weight_decay\":5e-4, \\\n","                          \"hidden\":16, \"dropout\":0.5, \"cuda\":True})\n","\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","if args.cuda:\n","    torch.cuda.manual_seed(args.seed)\n","\n","# Load data\n","adj, features, labels, idx_train, idx_val, idx_test = load_data()\n","# print('features', features.shape) # torch.Size([2708, 1433])\n","# print('labels', labels.shape)     # torch.Size([2708])\n","# print('adj',adj.shape)            # torch.Size([2708, 2708])\n","# Model and optimizer\n","model = GCN(nfeat=features.shape[1],          # 1433\n","            nhid=args.hidden,                 # 16\n","            nclass=labels.max().item() + 1,   # 7\n","            dropout=args.dropout)\n","optimizer = optim.Adam(model.parameters(),\n","                       lr=args.lr, weight_decay=args.weight_decay)\n","\n","if args.cuda:\n","    model.cuda()\n","    features = features.cuda()\n","    adj = adj.cuda()\n","    labels = labels.cuda()\n","    idx_train = idx_train.cuda()\n","    idx_val = idx_val.cuda()\n","    idx_test = idx_test.cuda()\n","\n","\n","def train(epoch):\n","    t = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    output = model(features, adj)\n","\n","    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n","    acc_train = accuracy(output[idx_train], labels[idx_train])\n","    loss_train.backward()\n","    optimizer.step()\n","\n","    if not args.fastmode:\n","        # Evaluate validation set performance separately,\n","        # deactivates dropout during validation run.\n","        model.eval()\n","        output = model(features, adj)\n","\n","    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n","    acc_val = accuracy(output[idx_val], labels[idx_val])\n","    print('Epoch: {:04d}'.format(epoch+1),\n","          'loss_train: {:.4f}'.format(loss_train.item()),\n","          'acc_train: {:.4f}'.format(acc_train.item()),\n","          'loss_val: {:.4f}'.format(loss_val.item()),\n","          'acc_val: {:.4f}'.format(acc_val.item()),\n","          'time: {:.4f}s'.format(time.time() - t))\n","\n","# Test\n","def test():\n","    model.eval()\n","    output = model(features, adj)\n","    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n","    acc_test = accuracy(output[idx_test], labels[idx_test])\n","    print(\"Test set results:\",\n","          \"loss= {:.4f}\".format(loss_test.item()),\n","          \"accuracy= {:.4f}\".format(acc_test.item()))\n","\n","\n","# Train model\n","t_total = time.time()\n","for epoch in range(args.epochs):\n","    train(epoch)\n","print(\"Optimization Finished!\")\n","print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n","\n","# Testing\n","test()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBwxO0WZlJOb","outputId":"6025960a-8d08-4821-9ea2-98975ff5fd8f","executionInfo":{"status":"ok","timestamp":1704780342001,"user_tz":-540,"elapsed":12992,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading cora dataset...\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-6-d28c1bd70d06>:90: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n","  return torch.sparse.FloatTensor(indices, values, shape)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: 0001 loss_train: 1.9772 acc_train: 0.1429 loss_val: 1.9649 acc_val: 0.1233 time: 1.5560s\n","Epoch: 0002 loss_train: 1.9572 acc_train: 0.1714 loss_val: 1.9504 acc_val: 0.1533 time: 0.0057s\n","Epoch: 0003 loss_train: 1.9435 acc_train: 0.1643 loss_val: 1.9357 acc_val: 0.1367 time: 0.0050s\n","Epoch: 0004 loss_train: 1.9337 acc_train: 0.1929 loss_val: 1.9204 acc_val: 0.1267 time: 0.0050s\n","Epoch: 0005 loss_train: 1.9164 acc_train: 0.1857 loss_val: 1.9046 acc_val: 0.1267 time: 0.0049s\n","Epoch: 0006 loss_train: 1.9030 acc_train: 0.2071 loss_val: 1.8888 acc_val: 0.3400 time: 0.0049s\n","Epoch: 0007 loss_train: 1.8852 acc_train: 0.2071 loss_val: 1.8733 acc_val: 0.3533 time: 0.0050s\n","Epoch: 0008 loss_train: 1.8691 acc_train: 0.3286 loss_val: 1.8580 acc_val: 0.3500 time: 0.0049s\n","Epoch: 0009 loss_train: 1.8578 acc_train: 0.3071 loss_val: 1.8428 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0010 loss_train: 1.8439 acc_train: 0.3071 loss_val: 1.8282 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0011 loss_train: 1.8287 acc_train: 0.3143 loss_val: 1.8142 acc_val: 0.3500 time: 0.0050s\n","Epoch: 0012 loss_train: 1.7976 acc_train: 0.3286 loss_val: 1.8011 acc_val: 0.3500 time: 0.0050s\n","Epoch: 0013 loss_train: 1.8014 acc_train: 0.3000 loss_val: 1.7890 acc_val: 0.3500 time: 0.0052s\n","Epoch: 0014 loss_train: 1.7954 acc_train: 0.3214 loss_val: 1.7779 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0015 loss_train: 1.7621 acc_train: 0.3500 loss_val: 1.7678 acc_val: 0.3500 time: 0.0050s\n","Epoch: 0016 loss_train: 1.7906 acc_train: 0.3000 loss_val: 1.7590 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0017 loss_train: 1.7617 acc_train: 0.3000 loss_val: 1.7510 acc_val: 0.3500 time: 0.0050s\n","Epoch: 0018 loss_train: 1.7499 acc_train: 0.3214 loss_val: 1.7437 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0019 loss_train: 1.7267 acc_train: 0.3000 loss_val: 1.7370 acc_val: 0.3500 time: 0.0049s\n","Epoch: 0020 loss_train: 1.7655 acc_train: 0.3143 loss_val: 1.7305 acc_val: 0.3500 time: 0.0062s\n","Epoch: 0021 loss_train: 1.7314 acc_train: 0.3143 loss_val: 1.7244 acc_val: 0.3500 time: 0.0055s\n","Epoch: 0022 loss_train: 1.7242 acc_train: 0.3000 loss_val: 1.7183 acc_val: 0.3500 time: 0.0055s\n","Epoch: 0023 loss_train: 1.7302 acc_train: 0.3071 loss_val: 1.7122 acc_val: 0.3500 time: 0.0050s\n","Epoch: 0024 loss_train: 1.6847 acc_train: 0.3143 loss_val: 1.7061 acc_val: 0.3500 time: 0.0049s\n","Epoch: 0025 loss_train: 1.7149 acc_train: 0.3071 loss_val: 1.6998 acc_val: 0.3500 time: 0.0051s\n","Epoch: 0026 loss_train: 1.6741 acc_train: 0.3071 loss_val: 1.6933 acc_val: 0.3500 time: 0.0069s\n","Epoch: 0027 loss_train: 1.6800 acc_train: 0.3000 loss_val: 1.6866 acc_val: 0.3500 time: 0.0049s\n","Epoch: 0028 loss_train: 1.6812 acc_train: 0.3357 loss_val: 1.6797 acc_val: 0.3533 time: 0.0049s\n","Epoch: 0029 loss_train: 1.6338 acc_train: 0.3571 loss_val: 1.6728 acc_val: 0.3533 time: 0.0071s\n","Epoch: 0030 loss_train: 1.6517 acc_train: 0.3286 loss_val: 1.6661 acc_val: 0.3633 time: 0.0073s\n","Epoch: 0031 loss_train: 1.6482 acc_train: 0.3786 loss_val: 1.6592 acc_val: 0.3667 time: 0.0051s\n","Epoch: 0032 loss_train: 1.6196 acc_train: 0.3714 loss_val: 1.6525 acc_val: 0.3700 time: 0.0051s\n","Epoch: 0033 loss_train: 1.6351 acc_train: 0.4000 loss_val: 1.6457 acc_val: 0.3733 time: 0.0052s\n","Epoch: 0034 loss_train: 1.6075 acc_train: 0.4214 loss_val: 1.6385 acc_val: 0.3800 time: 0.0050s\n","Epoch: 0035 loss_train: 1.5691 acc_train: 0.3857 loss_val: 1.6308 acc_val: 0.3967 time: 0.0049s\n","Epoch: 0036 loss_train: 1.6096 acc_train: 0.4000 loss_val: 1.6226 acc_val: 0.4033 time: 0.0052s\n","Epoch: 0037 loss_train: 1.5774 acc_train: 0.4214 loss_val: 1.6141 acc_val: 0.4100 time: 0.0051s\n","Epoch: 0038 loss_train: 1.5831 acc_train: 0.4214 loss_val: 1.6051 acc_val: 0.4200 time: 0.0050s\n","Epoch: 0039 loss_train: 1.5316 acc_train: 0.4500 loss_val: 1.5952 acc_val: 0.4267 time: 0.0050s\n","Epoch: 0040 loss_train: 1.5239 acc_train: 0.4571 loss_val: 1.5849 acc_val: 0.4167 time: 0.0051s\n","Epoch: 0041 loss_train: 1.5213 acc_train: 0.4357 loss_val: 1.5741 acc_val: 0.4133 time: 0.0050s\n","Epoch: 0042 loss_train: 1.4775 acc_train: 0.4357 loss_val: 1.5632 acc_val: 0.4133 time: 0.0050s\n","Epoch: 0043 loss_train: 1.4998 acc_train: 0.4286 loss_val: 1.5523 acc_val: 0.4133 time: 0.0050s\n","Epoch: 0044 loss_train: 1.4645 acc_train: 0.4571 loss_val: 1.5415 acc_val: 0.4167 time: 0.0050s\n","Epoch: 0045 loss_train: 1.4557 acc_train: 0.4429 loss_val: 1.5308 acc_val: 0.4300 time: 0.0049s\n","Epoch: 0046 loss_train: 1.4255 acc_train: 0.4429 loss_val: 1.5198 acc_val: 0.4333 time: 0.0050s\n","Epoch: 0047 loss_train: 1.4268 acc_train: 0.4429 loss_val: 1.5087 acc_val: 0.4433 time: 0.0049s\n","Epoch: 0048 loss_train: 1.4047 acc_train: 0.4643 loss_val: 1.4974 acc_val: 0.4467 time: 0.0051s\n","Epoch: 0049 loss_train: 1.3955 acc_train: 0.4643 loss_val: 1.4860 acc_val: 0.4533 time: 0.0049s\n","Epoch: 0050 loss_train: 1.3770 acc_train: 0.5143 loss_val: 1.4744 acc_val: 0.4600 time: 0.0049s\n","Epoch: 0051 loss_train: 1.3723 acc_train: 0.5000 loss_val: 1.4627 acc_val: 0.4667 time: 0.0049s\n","Epoch: 0052 loss_train: 1.3663 acc_train: 0.5143 loss_val: 1.4506 acc_val: 0.4733 time: 0.0051s\n","Epoch: 0053 loss_train: 1.3087 acc_train: 0.5071 loss_val: 1.4387 acc_val: 0.4833 time: 0.0055s\n","Epoch: 0054 loss_train: 1.3040 acc_train: 0.5429 loss_val: 1.4267 acc_val: 0.5067 time: 0.0051s\n","Epoch: 0055 loss_train: 1.2963 acc_train: 0.4714 loss_val: 1.4145 acc_val: 0.5133 time: 0.0067s\n","Epoch: 0056 loss_train: 1.3056 acc_train: 0.5286 loss_val: 1.4021 acc_val: 0.5200 time: 0.0049s\n","Epoch: 0057 loss_train: 1.2688 acc_train: 0.5143 loss_val: 1.3899 acc_val: 0.5233 time: 0.0050s\n","Epoch: 0058 loss_train: 1.2441 acc_train: 0.5500 loss_val: 1.3778 acc_val: 0.5267 time: 0.0083s\n","Epoch: 0059 loss_train: 1.2518 acc_train: 0.6143 loss_val: 1.3659 acc_val: 0.5300 time: 0.0050s\n","Epoch: 0060 loss_train: 1.2253 acc_train: 0.6143 loss_val: 1.3544 acc_val: 0.5633 time: 0.0050s\n","Epoch: 0061 loss_train: 1.2343 acc_train: 0.6071 loss_val: 1.3428 acc_val: 0.5700 time: 0.0050s\n","Epoch: 0062 loss_train: 1.1814 acc_train: 0.6429 loss_val: 1.3315 acc_val: 0.5767 time: 0.0050s\n","Epoch: 0063 loss_train: 1.1446 acc_train: 0.6786 loss_val: 1.3202 acc_val: 0.5833 time: 0.0049s\n","Epoch: 0064 loss_train: 1.1667 acc_train: 0.6857 loss_val: 1.3091 acc_val: 0.5900 time: 0.0051s\n","Epoch: 0065 loss_train: 1.1634 acc_train: 0.6571 loss_val: 1.2979 acc_val: 0.6067 time: 0.0050s\n","Epoch: 0066 loss_train: 1.1318 acc_train: 0.6929 loss_val: 1.2865 acc_val: 0.6333 time: 0.0050s\n","Epoch: 0067 loss_train: 1.1030 acc_train: 0.7357 loss_val: 1.2750 acc_val: 0.6400 time: 0.0053s\n","Epoch: 0068 loss_train: 1.1335 acc_train: 0.7143 loss_val: 1.2634 acc_val: 0.6600 time: 0.0050s\n","Epoch: 0069 loss_train: 1.0755 acc_train: 0.7357 loss_val: 1.2515 acc_val: 0.6667 time: 0.0049s\n","Epoch: 0070 loss_train: 1.0895 acc_train: 0.7429 loss_val: 1.2398 acc_val: 0.6667 time: 0.0049s\n","Epoch: 0071 loss_train: 1.0742 acc_train: 0.7214 loss_val: 1.2286 acc_val: 0.6700 time: 0.0049s\n","Epoch: 0072 loss_train: 1.0734 acc_train: 0.7357 loss_val: 1.2173 acc_val: 0.6767 time: 0.0049s\n","Epoch: 0073 loss_train: 1.0671 acc_train: 0.7500 loss_val: 1.2063 acc_val: 0.6867 time: 0.0048s\n","Epoch: 0074 loss_train: 1.0494 acc_train: 0.7786 loss_val: 1.1954 acc_val: 0.6933 time: 0.0048s\n","Epoch: 0075 loss_train: 1.0159 acc_train: 0.7786 loss_val: 1.1849 acc_val: 0.7100 time: 0.0050s\n","Epoch: 0076 loss_train: 0.9985 acc_train: 0.7929 loss_val: 1.1744 acc_val: 0.7267 time: 0.0049s\n","Epoch: 0077 loss_train: 1.0176 acc_train: 0.7714 loss_val: 1.1638 acc_val: 0.7433 time: 0.0055s\n","Epoch: 0078 loss_train: 0.9471 acc_train: 0.7857 loss_val: 1.1535 acc_val: 0.7467 time: 0.0050s\n","Epoch: 0079 loss_train: 0.9811 acc_train: 0.7929 loss_val: 1.1434 acc_val: 0.7533 time: 0.0048s\n","Epoch: 0080 loss_train: 0.9410 acc_train: 0.8214 loss_val: 1.1339 acc_val: 0.7567 time: 0.0049s\n","Epoch: 0081 loss_train: 0.9636 acc_train: 0.8143 loss_val: 1.1246 acc_val: 0.7600 time: 0.0048s\n","Epoch: 0082 loss_train: 0.9709 acc_train: 0.7929 loss_val: 1.1155 acc_val: 0.7567 time: 0.0049s\n","Epoch: 0083 loss_train: 0.8881 acc_train: 0.8357 loss_val: 1.1063 acc_val: 0.7567 time: 0.0048s\n","Epoch: 0084 loss_train: 0.9035 acc_train: 0.7857 loss_val: 1.0975 acc_val: 0.7633 time: 0.0048s\n","Epoch: 0085 loss_train: 0.9050 acc_train: 0.7929 loss_val: 1.0885 acc_val: 0.7700 time: 0.0048s\n","Epoch: 0086 loss_train: 0.9174 acc_train: 0.8214 loss_val: 1.0793 acc_val: 0.7700 time: 0.0049s\n","Epoch: 0087 loss_train: 0.9197 acc_train: 0.8214 loss_val: 1.0703 acc_val: 0.7767 time: 0.0059s\n","Epoch: 0088 loss_train: 0.8615 acc_train: 0.8429 loss_val: 1.0620 acc_val: 0.7733 time: 0.0077s\n","Epoch: 0089 loss_train: 0.8478 acc_train: 0.8643 loss_val: 1.0536 acc_val: 0.7767 time: 0.0049s\n","Epoch: 0090 loss_train: 0.8649 acc_train: 0.8286 loss_val: 1.0452 acc_val: 0.7767 time: 0.0068s\n","Epoch: 0091 loss_train: 0.8467 acc_train: 0.8714 loss_val: 1.0364 acc_val: 0.7767 time: 0.0051s\n","Epoch: 0092 loss_train: 0.8417 acc_train: 0.8429 loss_val: 1.0284 acc_val: 0.7733 time: 0.0050s\n","Epoch: 0093 loss_train: 0.7916 acc_train: 0.8357 loss_val: 1.0207 acc_val: 0.7800 time: 0.0048s\n","Epoch: 0094 loss_train: 0.8426 acc_train: 0.8286 loss_val: 1.0131 acc_val: 0.7800 time: 0.0049s\n","Epoch: 0095 loss_train: 0.8232 acc_train: 0.8357 loss_val: 1.0057 acc_val: 0.7800 time: 0.0048s\n","Epoch: 0096 loss_train: 0.7872 acc_train: 0.8714 loss_val: 0.9987 acc_val: 0.7800 time: 0.0049s\n","Epoch: 0097 loss_train: 0.7715 acc_train: 0.8643 loss_val: 0.9922 acc_val: 0.7767 time: 0.0051s\n","Epoch: 0098 loss_train: 0.7376 acc_train: 0.8357 loss_val: 0.9859 acc_val: 0.7767 time: 0.0048s\n","Epoch: 0099 loss_train: 0.7898 acc_train: 0.8714 loss_val: 0.9796 acc_val: 0.7800 time: 0.0048s\n","Epoch: 0100 loss_train: 0.7684 acc_train: 0.8571 loss_val: 0.9735 acc_val: 0.7800 time: 0.0048s\n","Epoch: 0101 loss_train: 0.7759 acc_train: 0.8857 loss_val: 0.9670 acc_val: 0.7800 time: 0.0048s\n","Epoch: 0102 loss_train: 0.7631 acc_train: 0.8571 loss_val: 0.9611 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0103 loss_train: 0.7501 acc_train: 0.8786 loss_val: 0.9546 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0104 loss_train: 0.7010 acc_train: 0.8857 loss_val: 0.9483 acc_val: 0.7933 time: 0.0051s\n","Epoch: 0105 loss_train: 0.7355 acc_train: 0.8357 loss_val: 0.9413 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0106 loss_train: 0.7836 acc_train: 0.8500 loss_val: 0.9349 acc_val: 0.7933 time: 0.0047s\n","Epoch: 0107 loss_train: 0.7327 acc_train: 0.8429 loss_val: 0.9294 acc_val: 0.7933 time: 0.0047s\n","Epoch: 0108 loss_train: 0.6996 acc_train: 0.8643 loss_val: 0.9246 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0109 loss_train: 0.6844 acc_train: 0.8714 loss_val: 0.9206 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0110 loss_train: 0.6742 acc_train: 0.8786 loss_val: 0.9172 acc_val: 0.7900 time: 0.0049s\n","Epoch: 0111 loss_train: 0.7161 acc_train: 0.8357 loss_val: 0.9136 acc_val: 0.7900 time: 0.0048s\n","Epoch: 0112 loss_train: 0.6856 acc_train: 0.8857 loss_val: 0.9099 acc_val: 0.7900 time: 0.0048s\n","Epoch: 0113 loss_train: 0.6832 acc_train: 0.8714 loss_val: 0.9063 acc_val: 0.7867 time: 0.0049s\n","Epoch: 0114 loss_train: 0.6770 acc_train: 0.8929 loss_val: 0.9018 acc_val: 0.7867 time: 0.0049s\n","Epoch: 0115 loss_train: 0.6804 acc_train: 0.8786 loss_val: 0.8973 acc_val: 0.7867 time: 0.0050s\n","Epoch: 0116 loss_train: 0.6625 acc_train: 0.8786 loss_val: 0.8925 acc_val: 0.7867 time: 0.0079s\n","Epoch: 0117 loss_train: 0.6829 acc_train: 0.8714 loss_val: 0.8880 acc_val: 0.7867 time: 0.0068s\n","Epoch: 0118 loss_train: 0.6674 acc_train: 0.8571 loss_val: 0.8834 acc_val: 0.7867 time: 0.0049s\n","Epoch: 0119 loss_train: 0.6492 acc_train: 0.8857 loss_val: 0.8787 acc_val: 0.7867 time: 0.0064s\n","Epoch: 0120 loss_train: 0.6359 acc_train: 0.8714 loss_val: 0.8744 acc_val: 0.7900 time: 0.0073s\n","Epoch: 0121 loss_train: 0.6867 acc_train: 0.8429 loss_val: 0.8714 acc_val: 0.7900 time: 0.0049s\n","Epoch: 0122 loss_train: 0.6022 acc_train: 0.9000 loss_val: 0.8688 acc_val: 0.7900 time: 0.0049s\n","Epoch: 0123 loss_train: 0.6013 acc_train: 0.8857 loss_val: 0.8662 acc_val: 0.7900 time: 0.0049s\n","Epoch: 0124 loss_train: 0.6309 acc_train: 0.9000 loss_val: 0.8636 acc_val: 0.7933 time: 0.0067s\n","Epoch: 0125 loss_train: 0.6489 acc_train: 0.8429 loss_val: 0.8604 acc_val: 0.7933 time: 0.0062s\n","Epoch: 0126 loss_train: 0.6370 acc_train: 0.8643 loss_val: 0.8564 acc_val: 0.7933 time: 0.0051s\n","Epoch: 0127 loss_train: 0.5953 acc_train: 0.8786 loss_val: 0.8521 acc_val: 0.7933 time: 0.0066s\n","Epoch: 0128 loss_train: 0.6092 acc_train: 0.9000 loss_val: 0.8478 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0129 loss_train: 0.6505 acc_train: 0.8500 loss_val: 0.8442 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0130 loss_train: 0.5903 acc_train: 0.8929 loss_val: 0.8409 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0131 loss_train: 0.5746 acc_train: 0.8929 loss_val: 0.8381 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0132 loss_train: 0.6161 acc_train: 0.8714 loss_val: 0.8350 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0133 loss_train: 0.6054 acc_train: 0.8786 loss_val: 0.8326 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0134 loss_train: 0.5738 acc_train: 0.8643 loss_val: 0.8308 acc_val: 0.7933 time: 0.0050s\n","Epoch: 0135 loss_train: 0.5981 acc_train: 0.8857 loss_val: 0.8293 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0136 loss_train: 0.5981 acc_train: 0.8857 loss_val: 0.8276 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0137 loss_train: 0.5563 acc_train: 0.8929 loss_val: 0.8243 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0138 loss_train: 0.5990 acc_train: 0.8786 loss_val: 0.8201 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0139 loss_train: 0.5847 acc_train: 0.9000 loss_val: 0.8153 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0140 loss_train: 0.5501 acc_train: 0.8714 loss_val: 0.8111 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0141 loss_train: 0.5838 acc_train: 0.8857 loss_val: 0.8083 acc_val: 0.7967 time: 0.0049s\n","Epoch: 0142 loss_train: 0.5514 acc_train: 0.8857 loss_val: 0.8057 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0143 loss_train: 0.5434 acc_train: 0.9286 loss_val: 0.8033 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0144 loss_train: 0.5875 acc_train: 0.8857 loss_val: 0.8015 acc_val: 0.7933 time: 0.0079s\n","Epoch: 0145 loss_train: 0.5394 acc_train: 0.8929 loss_val: 0.7995 acc_val: 0.7933 time: 0.0056s\n","Epoch: 0146 loss_train: 0.5607 acc_train: 0.8929 loss_val: 0.7981 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0147 loss_train: 0.5153 acc_train: 0.9143 loss_val: 0.7956 acc_val: 0.7933 time: 0.0050s\n","Epoch: 0148 loss_train: 0.5448 acc_train: 0.9071 loss_val: 0.7929 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0149 loss_train: 0.5426 acc_train: 0.9071 loss_val: 0.7892 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0150 loss_train: 0.5288 acc_train: 0.9214 loss_val: 0.7860 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0151 loss_train: 0.5746 acc_train: 0.8857 loss_val: 0.7830 acc_val: 0.7933 time: 0.0051s\n","Epoch: 0152 loss_train: 0.5440 acc_train: 0.9000 loss_val: 0.7811 acc_val: 0.7933 time: 0.0050s\n","Epoch: 0153 loss_train: 0.5112 acc_train: 0.8786 loss_val: 0.7796 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0154 loss_train: 0.5793 acc_train: 0.8786 loss_val: 0.7773 acc_val: 0.7900 time: 0.0049s\n","Epoch: 0155 loss_train: 0.5081 acc_train: 0.9071 loss_val: 0.7752 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0156 loss_train: 0.4585 acc_train: 0.9214 loss_val: 0.7728 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0157 loss_train: 0.5120 acc_train: 0.9286 loss_val: 0.7707 acc_val: 0.7933 time: 0.0049s\n","Epoch: 0158 loss_train: 0.4915 acc_train: 0.9214 loss_val: 0.7689 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0159 loss_train: 0.5404 acc_train: 0.9000 loss_val: 0.7671 acc_val: 0.7967 time: 0.0050s\n","Epoch: 0160 loss_train: 0.5505 acc_train: 0.8643 loss_val: 0.7661 acc_val: 0.7967 time: 0.0062s\n","Epoch: 0161 loss_train: 0.5062 acc_train: 0.9071 loss_val: 0.7654 acc_val: 0.7967 time: 0.0081s\n","Epoch: 0162 loss_train: 0.5108 acc_train: 0.9071 loss_val: 0.7650 acc_val: 0.7967 time: 0.0051s\n","Epoch: 0163 loss_train: 0.5255 acc_train: 0.9000 loss_val: 0.7638 acc_val: 0.7967 time: 0.0049s\n","Epoch: 0164 loss_train: 0.5062 acc_train: 0.9071 loss_val: 0.7621 acc_val: 0.7967 time: 0.0052s\n","Epoch: 0165 loss_train: 0.5367 acc_train: 0.8929 loss_val: 0.7598 acc_val: 0.7967 time: 0.0049s\n","Epoch: 0166 loss_train: 0.4949 acc_train: 0.9000 loss_val: 0.7564 acc_val: 0.7967 time: 0.0049s\n","Epoch: 0167 loss_train: 0.4814 acc_train: 0.9214 loss_val: 0.7530 acc_val: 0.8067 time: 0.0049s\n","Epoch: 0168 loss_train: 0.5006 acc_train: 0.8786 loss_val: 0.7501 acc_val: 0.8067 time: 0.0049s\n","Epoch: 0169 loss_train: 0.4851 acc_train: 0.9071 loss_val: 0.7480 acc_val: 0.8067 time: 0.0048s\n","Epoch: 0170 loss_train: 0.5149 acc_train: 0.9000 loss_val: 0.7462 acc_val: 0.8067 time: 0.0049s\n","Epoch: 0171 loss_train: 0.4862 acc_train: 0.9000 loss_val: 0.7450 acc_val: 0.8067 time: 0.0049s\n","Epoch: 0172 loss_train: 0.4781 acc_train: 0.9000 loss_val: 0.7442 acc_val: 0.8067 time: 0.0074s\n","Epoch: 0173 loss_train: 0.4762 acc_train: 0.9143 loss_val: 0.7450 acc_val: 0.8067 time: 0.0078s\n","Epoch: 0174 loss_train: 0.4473 acc_train: 0.9357 loss_val: 0.7476 acc_val: 0.8000 time: 0.0048s\n","Epoch: 0175 loss_train: 0.4624 acc_train: 0.9071 loss_val: 0.7503 acc_val: 0.7967 time: 0.0050s\n","Epoch: 0176 loss_train: 0.4738 acc_train: 0.9143 loss_val: 0.7517 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0177 loss_train: 0.4941 acc_train: 0.9214 loss_val: 0.7510 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0178 loss_train: 0.4761 acc_train: 0.9357 loss_val: 0.7490 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0179 loss_train: 0.4712 acc_train: 0.9071 loss_val: 0.7450 acc_val: 0.8000 time: 0.0049s\n","Epoch: 0180 loss_train: 0.4837 acc_train: 0.9071 loss_val: 0.7404 acc_val: 0.8000 time: 0.0049s\n","Epoch: 0181 loss_train: 0.4821 acc_train: 0.9143 loss_val: 0.7374 acc_val: 0.7967 time: 0.0047s\n","Epoch: 0182 loss_train: 0.4764 acc_train: 0.9214 loss_val: 0.7358 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0183 loss_train: 0.4410 acc_train: 0.9214 loss_val: 0.7354 acc_val: 0.7933 time: 0.0047s\n","Epoch: 0184 loss_train: 0.4555 acc_train: 0.9143 loss_val: 0.7357 acc_val: 0.7900 time: 0.0047s\n","Epoch: 0185 loss_train: 0.4895 acc_train: 0.8857 loss_val: 0.7349 acc_val: 0.7933 time: 0.0050s\n","Epoch: 0186 loss_train: 0.4301 acc_train: 0.9357 loss_val: 0.7345 acc_val: 0.8000 time: 0.0050s\n","Epoch: 0187 loss_train: 0.5104 acc_train: 0.8643 loss_val: 0.7327 acc_val: 0.8033 time: 0.0051s\n","Epoch: 0188 loss_train: 0.4613 acc_train: 0.9143 loss_val: 0.7323 acc_val: 0.8067 time: 0.0048s\n","Epoch: 0189 loss_train: 0.4550 acc_train: 0.9357 loss_val: 0.7316 acc_val: 0.8033 time: 0.0048s\n","Epoch: 0190 loss_train: 0.4766 acc_train: 0.9071 loss_val: 0.7311 acc_val: 0.8033 time: 0.0048s\n","Epoch: 0191 loss_train: 0.4751 acc_train: 0.9214 loss_val: 0.7289 acc_val: 0.8033 time: 0.0051s\n","Epoch: 0192 loss_train: 0.4570 acc_train: 0.9000 loss_val: 0.7277 acc_val: 0.8033 time: 0.0075s\n","Epoch: 0193 loss_train: 0.4709 acc_train: 0.9071 loss_val: 0.7264 acc_val: 0.7967 time: 0.0048s\n","Epoch: 0194 loss_train: 0.4754 acc_train: 0.9429 loss_val: 0.7264 acc_val: 0.7967 time: 0.0049s\n","Epoch: 0195 loss_train: 0.4317 acc_train: 0.9286 loss_val: 0.7256 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0196 loss_train: 0.4134 acc_train: 0.9500 loss_val: 0.7245 acc_val: 0.7900 time: 0.0048s\n","Epoch: 0197 loss_train: 0.4569 acc_train: 0.9000 loss_val: 0.7225 acc_val: 0.7900 time: 0.0048s\n","Epoch: 0198 loss_train: 0.4718 acc_train: 0.8857 loss_val: 0.7209 acc_val: 0.7900 time: 0.0048s\n","Epoch: 0199 loss_train: 0.4091 acc_train: 0.9357 loss_val: 0.7189 acc_val: 0.7933 time: 0.0048s\n","Epoch: 0200 loss_train: 0.4428 acc_train: 0.9357 loss_val: 0.7170 acc_val: 0.7967 time: 0.0049s\n","Optimization Finished!\n","Total time elapsed: 2.9881s\n","Test set results: loss= 0.7485 accuracy= 0.8100\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Z0e2BHaWINnQ","executionInfo":{"status":"aborted","timestamp":1704779025521,"user_tz":-540,"elapsed":2,"user":{"displayName":"정지원","userId":"16823124283758121408"}}},"execution_count":null,"outputs":[]}]}